{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas\n",
    "\n",
    "from unityagents import UnityEnvironment\n",
    "from DoubleQLearner import Agent\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Banana.app\"`\n",
    "- **Windows** (x86): `\"path/to/Banana_Windows_x86/Banana.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Banana_Windows_x86_64/Banana.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Banana_Linux/Banana.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Banana_Linux/Banana.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Banana.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Banana.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialize environment object\n",
    "import os\n",
    "env_name = 'banana_collector'\n",
    "env = UnityEnvironment(file_name=os.environ['HOME']+\"/ML/deep-reinforcement-learning/p1_navigation/Banana_Linux/Banana.x86_64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "The simulation contains a single agent that navigates a large environment.  At each time step, it has four actions at its disposal:\n",
    "- `0` - walk forward \n",
    "- `1` - walk backward\n",
    "- `2` - turn left\n",
    "- `3` - turn right\n",
    "\n",
    "The state space has `37` dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction.  A reward of `+1` is provided for collecting a yellow banana, and a reward of `-1` is provided for collecting a blue banana. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# Get environment parameter\n",
    "number_of_agents = len(env_info.agents)\n",
    "action_size = brain.vector_action_space_size\n",
    "state_size = len(env_info.vector_observations[0])\n",
    "print('Number of agents  : ', number_of_agents)\n",
    "print('Number of actions : ', action_size)\n",
    "print('Dimension of state space : ', state_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train Double-DQN agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Double-DQN agent\n",
    "agent = Agent(name='ddqn', state_size=state_size, action_size=action_size, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters for training\n",
    "episodes = 1800                     # maximum number of training episodes\n",
    "\n",
    "# Define parameters for e-Greedy policy\n",
    "epsilon = 1.0                       # starting value of epsilon\n",
    "epsilon_floor = 0.05                # minimum value of epsilon\n",
    "epsilon_decay = 0.993               # factor for decreasing epsilon\n",
    "\n",
    "# Training loop\n",
    "scores = []                         # list containing scores from each episode\n",
    "scores_window = deque(maxlen=100)   # last 100 scores\n",
    "for i_episode in range(1, episodes+1):\n",
    "    # Reset the environment\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "    # Capture the current state\n",
    "    state = env_info.vector_observations[0]\n",
    "\n",
    "    # Reset score collector\n",
    "    score = 0\n",
    "    done = False\n",
    "    # One episode loop\n",
    "    while not done:\n",
    "        # Action selection by Epsilon-Greedy policy\n",
    "        action = agent.act(state, epsilon)\n",
    "\n",
    "        # Take action and get rewards and new state\n",
    "        env_info = env.step(action)[brain_name]\n",
    "        next_state = env_info.vector_observations[0]\n",
    "        reward = env_info.rewards[0]\n",
    "        done = env_info.local_done[0]                  # if next is terminal state\n",
    "\n",
    "        # Store experience\n",
    "        agent.step(state, action, reward, next_state, done)\n",
    "\n",
    "        # State transition\n",
    "        state = next_state\n",
    "\n",
    "        # Update total score\n",
    "        score += reward\n",
    "\n",
    "    # Push to score list\n",
    "    scores_window.append(score)\n",
    "    scores.append([score, np.mean(scores_window)])\n",
    "\n",
    "    # Print episode summary\n",
    "    print('\\r#TRAIN Episode:{}, Score:{:.2f}, Average Score:{:.2f}, Exploration:{:1.4f}'.format(i_episode, score, np.mean(scores_window), epsilon), end=\"\")\n",
    "    if i_episode % 100 == 0:\n",
    "        print('\\r#TRAIN Episode:{}, Score:{:.2f}, Average Score:{:.2f}, Exploration:{:1.4f}'.format(i_episode, score, np.mean(scores_window), epsilon))\n",
    "    if np.mean(scores_window)>=13.0:\n",
    "        print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "        torch.save(agent.Q_network.state_dict(), 'models/%s_%s.pth'% (agent.name, env_name))\n",
    "        break\n",
    "\n",
    "    # Update exploration\n",
    "    epsilon = max(epsilon_floor, epsilon*epsilon_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export scores to csv file\n",
    "df = pandas.DataFrame(scores,columns=[\"scores\",\"average_scores\"])\n",
    "#df = pandas.DataFrame(data={\"score\": scores[0],\"average_score\": scores[1]})\n",
    "df.to_csv('scores/%s_%s_trained_%d_episodes.csv'% (agent.name, env_name, i_episode), sep=',',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "ax.legend(['Raw scores','Average scores'])\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Test Double-DQN agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained network\n",
    "agent.Q_network.load_state_dict(torch.load('models/%s_%s.pth'% (agent.name, env_name)))\n",
    "\n",
    "# Define parameters for test\n",
    "episodes = 2                        # maximum number of test episodes\n",
    "\n",
    "# Test loop\n",
    "for i_episode in range(1, episodes+1):\n",
    "    # Reset the environment\n",
    "    env_info = env.reset(train_mode=False)[brain_name]\n",
    "\n",
    "    # Capture the current state\n",
    "    state = env_info.vector_observations[0]\n",
    "\n",
    "    # Reset score collector\n",
    "    score = 0\n",
    "    done = False\n",
    "    # One episode loop\n",
    "    while not done:\n",
    "        # Action selection by Epsilon-Greedy policy\n",
    "        action = agent.act(state)\n",
    "\n",
    "        # Take action and get rewards and new state\n",
    "        env_info = env.step(action)[brain_name]\n",
    "        next_state = env_info.vector_observations[0]\n",
    "        reward = env_info.rewards[0]\n",
    "        done = env_info.local_done[0]              # if next is terminal state\n",
    "\n",
    "        # State transition\n",
    "        state = next_state\n",
    "\n",
    "        # Update total score\n",
    "        score += reward\n",
    "\n",
    "    # Print episode summary\n",
    "    print('\\r#TEST Episode:{}, Score:{:.2f}'.format(i_episode, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
